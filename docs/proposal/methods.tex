\section{Methods}\label{sec:methods}

Mikolov et al. proposed two distinct to compute word vectors: the Continuous Bag-of-Words (CBOW) model and the Skip-gram model. 
Both models consist of a projection layer followed by a hierarchical softmax output layer. 
The only nonlinearity is the exponentiation in the output layer. 
The two models have slightly different objective. 
Given a sequence of words, $w_0, \hdots , w_k, \hdots, w_{k+k}$, the CBOW model uses $w_0, \hdots, w_{k-1}, \hdots, w_{k+1}, \hdots, w_{k+1}$, to predict $w_k$. 
The input words are represented as one-hot vectors and are all projected down into the same position by averaging the projected vectors. 
Note that this means all information about location of the words in the sentence is lost (hence the name “bag of words”). 
The projection layer acts as input to a hierarchical softmax layer that aims to project the remaining word $w_k$. 
The Skip-gram model takes a single word as input and aims to predict the $k$ previous and $k$ future words. 
The performance of both models is comparable.

	The thesis of subspace clustering algorithms is that points in a higher dimensional space are generated by projections from a lower dimensional space. 
The clustering algorithm aims to recover the number of subspaces and their dimensionalities along with the basis for the subspace that every point in the original space is deemed to lie in. 
We will be using a subspace clustering algorithm based on spectral clustering. We use this approach since it does not require us to specify the dimensionality of the subspaces in advance.
