\section{Methods}\label{sec:methods}

Mikolov et al. proposed two distinct to compute word vectors: the Continuous Bag-of-Words (CBOW) model and the Skip-gram model. 
Both models consist of a projection layer followed by a hierarchical softmax output layer. 
The only nonlinearity is the exponentiation in the output layer. 
The two models have slightly different objective. 
Given a sequence of words, $w_0, \hdots , w_k, \hdots, w_{k+k}$, the CBOW model uses $w_0, \hdots, w_{k-1}, \hdots, w_{k+1}, \hdots, w_{k+k}$, to predict $w_k$. 
The input words are represented as one-hot vectors and are all projected down into the same position by averaging the projected vectors. 
Note that this means all information about location of the words in the sentence is lost (hence the name `bag of words'). 
The projection layer acts as input to a hierarchical softmax layer that aims to project the remaining word $w_k$. 
The Skip-gram model takes a single word as input and aims to predict the $k$ previous and $k$ future words. 
The performance of both models is comparable.

Subspace clustering is useful in cases where you want to model a collection of data points that belong to a union of several subspaces.
There are a variety of algorithms for subspace clustering. The general aim is to recover the number of subspaces and their dimensionalities along with a basis for each subspace and an assignment of each point to a subspace. 
We will be using a subspace clustering algorithm based on spectral clustering. We use this approach since it does not require us to specify the dimensionality of the subspaces in advance. Rather, we can use the cluster assignments of each point to compute each subspace via SVD decomposition, testing out a variety of ranks. 
