Mikolov et al. proposed two distinct to compute word vectors: the Continuous Bag-of-Words (CBOW) model and the Skip-gram model. 
Both models consist of a projection layer followed by a hierarchical softmax output layer. 
The only nonlinearity is the exponentiation in the output layer. 
The two models have slightly different objective. 
Given a sequence of words, $w_0, \hdots , w_k, \hdots, w_{k+k}$, the CBOW model uses $w_0, \hdots, w_{k-1}, \hdots, w_{k+1}, \hdots, w_{k+1}$, to predict $w_k$. 
The input words are represented as 1-hot vectors and are all projected down into the same position by averaging the projected vectors. 
Note that this means all information about location of the words in the sentence is lost (hence the name “bag of words”). 
The projection layer acts as input to a hierarchical softmax layer that aims to project the remaining word $w_k$. 
The Skip-gram model takes a single word as input and aims to predict the $k$ previous and $k$ future words. 
Performance of both models is comparable.

	The thesis of subspace clustering algorithms is that points in a higher dimensional space are generated by projections from a lower dimensional space. The clustering algorithm aims to recover the number of subspaces and their dimensionalities along with the basis for the subspace that every point in the original space is deemed to lie in. 
