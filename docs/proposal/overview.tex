Recently, neural network based models for computing continuous vector representations of words have gained popularity. 
In particular, Mikolov and colleagues [1, 2, 3] have proposed two new models for learning a continuous embedding space for words from raw text data. 
Their models are similar to the neural probabilistic language model [5] and recurrent neural networks models [6]. 
However, Mikolov’s models are log linear, and thus have a much lower computational complexity. 
Furthermore, their models are trained on billions of words, which is several orders of magnitude larger than anything previously trained.
 
Mikolov et al. showed that the embedding space learned from these models has very interesting linear structure that can be exploited to solve a variety of language tasks. 
They showed that analogical questions of the form “King is to man as Queen is to \_\_\_\_” can be solved by simple algebraic operations on the vector representations. 
This particular question would be solved by computing the vector King - man + Queen and then searching for the nearest word vector. 
The result should be woman. 
After training with the CBOW and Skip-gram models, these types of analogical questions (both semantic and syntactic) are able to be answered correctly using this vector offset method. 
They have also shown that embedding spaces that result for different languages have similar geometrical structure and thus words can be translated from language A to language B by learning a simple linear transformation from the embedding space for language A to the embedding space for language B.
 
In our project we will be exploring various aspects of the linear structure of these embedding spaces. 
Our hypothesis is that words belonging to a particular superclass, or said differently, words that are all related in a particular context, might have some low dimensional structure. 
For example, it might be the case that words denoting fruit such as “apple”,”orange”,”pear” and “banana” all lie close to a lower dimensional manifold. 
To be clear, we are suggesting something stronger than just the claim that these points should all cluster together. 
We already know that the latter should be the case. 
Given the linear structure that appears to be present in the embedding space, we further hypothesize that these manifolds will in fact be hyperplanes. 
As a result we will be approaching this problem as a subspace clustering problem, rather than a general manifold learning problem.
